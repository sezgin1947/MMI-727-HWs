{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "hw2_Question 3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYv9MueCwsn",
        "colab_type": "text"
      },
      "source": [
        "# 3. Convolutional Neural Networks (60 pts) VERSION 1 M.SEZGIN BALOGLU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpk-UgW9Cwsn",
        "colab_type": "text"
      },
      "source": [
        "### Implement the convolutional neural network shown below for CIFAR-10 dataset.  Your code must follow these rules:\n",
        "\n",
        "__- Use your own implementation.__\n",
        "\n",
        "__- You can only use \" tf \" and \" tf.nn \" libraries for version 1 (40 pts). Check [tf.nn](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/nn)__\n",
        "\n",
        "__- You can use \" tf.layers \" library for version 2 (20 pts). Check [tf.layers](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)__\n",
        "\n",
        "__- Use ReLU activation function__\n",
        "\n",
        "__- Use batch normalization for convolutional layers__\n",
        "\n",
        "__- Use dropout for fully connected layers (for training only)__\n",
        "\n",
        "__- Write necessary explanations for each cell. Explanations should be detailed.__\n",
        "\n",
        "__- You can use codes from Lab Notebook.__\n",
        "\n",
        "__- (OPTIONAL) You can add more layers or use different methods for better accuracy. If you want, send an another notebook file with better accuracy for bonus points.__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5bOUxQKCwso",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/METU-MMI-DeepLearning/MMI713_Deep_Learning/blob/master/Assignment%202/model.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE3ltKqWCwsp",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqgg8OMlCwsp",
        "colab_type": "code",
        "outputId": "b19c4bdc-d939-4a11-9461-fafc2d909043",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEOoMK_ZCwsr",
        "colab_type": "text"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "You can normalize input data \"x\" if needed. You can use \"np.squeeze\" to remove single-dimensional entries from the shape of an array. You probably need to convert labels \"y\" to one-hot vector for loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YL3QdYjCwsr",
        "colab_type": "code",
        "outputId": "5a8e1311-7d07-40e2-f011-65268d4d56ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(x_train.shape)\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255   # place input values between 0-1\n",
        "\n",
        "y_train = tf.one_hot(np.squeeze(y_train),10)  #implement one-hot dec. for labels\n",
        "y_test = tf.one_hot(np.squeeze(y_test),10)\n",
        "y_train_cls = tf.argmax(y_train, axis=1)  #get the classes as single guesses\n",
        "y_test_cls = tf.argmax(y_test, axis=1)\n",
        "\n",
        "session = tf.Session()  \n",
        "y_train = y_train.eval(session=session) #datatype conversion from tensor to np.ndarray\n",
        "y_train_cls = y_train_cls.eval(session=session)\n",
        "y_test = y_test.eval(session=session)\n",
        "y_test_cls = y_test_cls.eval(session=session)\n",
        "\n",
        "x_train = x_train.astype(np.float32)  #placeholders are float32 hence, convert them\n",
        "x_test = x_test.astype(np.float32)\n",
        "print(type(x_train[0,1,1,1])) #debug prints\n",
        "print(type(x_test[0,1,17,1]))\n",
        "print(type(y_train[0,1]))\n",
        "print(type(y_test[0,1]))\n",
        "print(type(y_test_cls[0]))\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(y_test_cls.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "<class 'numpy.float32'>\n",
            "<class 'numpy.float32'>\n",
            "<class 'numpy.float32'>\n",
            "<class 'numpy.float32'>\n",
            "<class 'numpy.int64'>\n",
            "(50000, 10)\n",
            "(10000, 10)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7gS1DR4Cwst",
        "colab_type": "text"
      },
      "source": [
        "## Define placeholders\n",
        "\n",
        "Define input and output placeholders. It is a good idea to pass dropout rate also as a placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2NkMH2HCwsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder(tf.float32, [None, 32,32,3]) #placeholder for input\n",
        "y_true = tf.placeholder(tf.float32, [None, 10]) #placeholder for label in order to use in error mtrx. calc.\n",
        "y_true_cls = tf.placeholder(tf.int64, [None]) #placeholder for class guesses\n",
        "dropout_rate = tf.placeholder(tf.float32) #dropout rate\n",
        "test_training = tf.placeholder(tf.bool)  # flag for test and training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRPDkWo4Cwsv",
        "colab_type": "text"
      },
      "source": [
        "## Define variables\n",
        "\n",
        "Define filters for convolutional layers, weights and biases for fully connected layers. Don't forget to use a initializer. Xavier is recommended.\n",
        "\n",
        "\n",
        "*VARIABLES ARE DEFINED IN BELOW FUNCTION WHERE I CREATED THE NETWORK. SORRY FOR BAD CODE, RE-ARRANGED IT TOO MUCH COULDNT FIND TIME TO PROPERLY RE-WRITE*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D7ZiydvCwsx",
        "colab_type": "text"
      },
      "source": [
        "## Define network\n",
        "\n",
        "Define the network as a function.Recommended functions are:\n",
        "\n",
        "`\n",
        "tf.nn.convolution / tf.nn.pool / tf.nn.batch_normalization / tf.nn.dropout / tf.reshape`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uglLHBb7Cwsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def netw0rk(x, dropout_rate ,\n",
        "            conv1 = tf.get_variable(\"conv1\", shape=[5,5,3,32], initializer=tf.glorot_normal_initializer()), \n",
        "padding_conv1 = \"SAME\",\n",
        "stride_conv1 = [1,1],  #1st conv layer, 5x5x3 filters total of 32 filters\n",
        "\n",
        "window1 = [3,3],   #1st pooling layer\n",
        "padding_pool1 =\"SAME\",\n",
        "pooling_type1 = \"AVG\",\n",
        "stride_pool1 =  [2,2],  # Stride amount is 2 in each direction\n",
        "\n",
        "conv2 = tf.get_variable(\"conv2\", shape=[5,5,32,16], initializer=tf.glorot_normal_initializer()),\n",
        "padding_conv2 = \"SAME\",\n",
        "stride_conv2 = [1,1],  #2nd conv layer\n",
        "\n",
        "window2 = [3,3],\n",
        "padding_pool2 = \"SAME\",\n",
        "pooling_type2 = \"AVG\",\n",
        "stride_pool2 = [2,2],  #2nd pooling layer\n",
        "\n",
        "conv3 = tf.get_variable(\"conv3\", shape=[5,5,16,32], initializer=tf.glorot_normal_initializer()),\n",
        "padding_conv3 = \"SAME\",\n",
        "stride_conv3 = [1,1],  #3rd conv layer\n",
        "\n",
        "window3 = [3,3],\n",
        "padding_pool3 = \"SAME\",\n",
        "pooling_type3 = \"AVG\",\n",
        "stride_pool3 = [2,2],  #3rd pooling layer\n",
        "\n",
        "\n",
        "weights1 = tf.Variable(tf.random.uniform([32*4*4,128])), #weights for 1st fc layer, randomly initialized with random dist.\n",
        "weights2 = tf.Variable(tf.random.uniform([128,10])), # weights for 2nd fc layer\n",
        "bias1 = tf.Variable(tf.zeros([128])),  #biases of first fc layer, zero initialized.\n",
        "bias2 = tf.Variable(tf.zeros([10]))   # biases of second fc layer\n",
        "):\n",
        "# LAYER 1\n",
        "    conv_layer1 = tf.nn.convolution(input = x, filter= conv1, padding= padding_conv1,\n",
        "                                      strides = stride_conv1 )  #1st convolution\n",
        "    mean1, variance1 = tf.nn.moments(conv_layer1,[0]) #calculation of batch norm. parameters\n",
        "    scale1 = tf.Variable(tf.ones([32,32,32])) # calculation of batch norm. parameters.\n",
        "    beta1 = tf.Variable(tf.zeros([32,32,32]))\n",
        "    bn1 = tf.nn.batch_normalization(x =conv_layer1, mean=mean1, variance= variance1,  #batch norm.\n",
        "                                    scale= scale1, offset=beta1, variance_epsilon=0.0000001)\n",
        "    relu_layer1 = tf.nn.relu(bn1)   # 1st ReLU layer, activation function\n",
        "    pool_layer1 = tf.nn.pool(input = relu_layer1, window_shape = window1, pooling_type = pooling_type1, \n",
        "                              padding = padding_pool1, strides = stride_pool1)  #1st pooling layer\n",
        "# LAYER 2 layer 2 and 3 are not commented again since they have same structure with layer 1.\n",
        "    conv_layer2 = tf.nn.convolution(input = pool_layer1, filter= conv2, padding= padding_conv2,\n",
        "                                      strides = stride_conv2 )\n",
        "    mean2, variance2 = tf.nn.moments(conv_layer2,[0])\n",
        "    scale2 = tf.Variable(tf.ones([16,16,16]))\n",
        "    beta2 = tf.Variable(tf.zeros([16,16,16]))\n",
        "    bn2 = tf.nn.batch_normalization(x =conv_layer2, mean=mean2, variance= variance2,\n",
        "                                    scale= scale2, offset=beta2, variance_epsilon=0.0000001)\n",
        "    relu_layer2 = tf.nn.relu(bn2)\n",
        "    pool_layer2 = tf.nn.pool(input = relu_layer2, window_shape = window2, pooling_type = pooling_type2, \n",
        "                              padding = padding_pool2, strides = stride_pool2)\n",
        "# LAYER 3\n",
        "    conv_layer3 = tf.nn.convolution(input = pool_layer2, filter= conv3, padding= padding_conv3,\n",
        "                                      strides = stride_conv3 )\n",
        "    mean3, variance3 = tf.nn.moments(conv_layer3,[0])\n",
        "    scale3 = tf.Variable(tf.ones([8,8,32]))\n",
        "    beta3 = tf.Variable(tf.zeros([8,8,32]))\n",
        "    bn3 = tf.nn.batch_normalization(x =conv_layer3, mean=mean3, variance= variance3,\n",
        "                                    scale= scale3, offset=beta3, variance_epsilon=0.0000001)\n",
        "    relu_layer3 = tf.nn.relu(bn3)\n",
        "    pool_layer3 = tf.nn.pool(input = relu_layer3, window_shape = window3, pooling_type = pooling_type3, \n",
        "                              padding = padding_pool3, strides = stride_pool3)\n",
        "\n",
        "    pool_layer3_x = tf.reshape(pool_layer3, [-1,32*4*4])  #flatten for fc layer\n",
        "\n",
        "  # 1st FC Layer\n",
        "    fc_layer = tf.matmul(pool_layer3_x, weights1) + bias1\n",
        "    fc_layer_x = tf.nn.dropout(fc_layer, rate = dropout_rate)\n",
        "    aa = tf.nn.relu(fc_layer_x)\n",
        "    #2nd fc layer\n",
        "    result = tf.matmul(aa, weights2) + bias2\n",
        "    return result\n",
        "\n",
        "logits = netw0rk(x, dropout_rate)  \n",
        "y_pred = tf.nn.softmax(logits)  #calculate output pred.\n",
        "y_pred_cls = tf.argmax(y_pred, axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYHUDRlVCwsz",
        "colab_type": "text"
      },
      "source": [
        "## Define cost function\n",
        "\n",
        "Define cost with respect to predictions and labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3y1d0C0Cwsz",
        "colab_type": "code",
        "outputId": "c32c35c2-b049-4694-f7ad-bda029defda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,    #calc. loss between result and labels\n",
        "                                                        labels=y_true)\n",
        "cost = tf.reduce_mean(cross_entropy)  # cost function."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-bd25a9c2f912>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51EwRVaNCws1",
        "colab_type": "text"
      },
      "source": [
        "## Define optimizer\n",
        "\n",
        "Adam is recommended"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7_ZeOlTCws1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.005).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9D4Ook2Cws2",
        "colab_type": "text"
      },
      "source": [
        "## Define performance measures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDNLQR0_Cws3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #data flow elements for perf. measurement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CWFb6DECws4",
        "colab_type": "text"
      },
      "source": [
        "## Create TensorFlow session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxmAkz_2Cws5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#session = tf.Session() # commented since a session is already started in 1st cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiFHxELICws7",
        "colab_type": "text"
      },
      "source": [
        "## Initialize variables\n",
        "\n",
        "The variables for `weights`,`filters` and `biases` must be initialized before we start optimizing them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XV9pysJCws7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_36zpCCws9",
        "colab_type": "text"
      },
      "source": [
        "## Define train function\n",
        "\n",
        "Don't forget to use batches since dataset is large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q6PSUYpCws9",
        "colab_type": "text"
      },
      "source": [
        "Hint : One of the simplest way to define batch is\n",
        "\n",
        "`for b in range(dataset_size//batch_size):      \n",
        "    x_batch = x_train[b * batch_size : (b+1) * batch_size]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDrXnOiDE-TO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(batch_size = 100):\n",
        "    correct_pred_array = []\n",
        "    for b in range(x_train.shape[0]//batch_size):      \n",
        "        x_batch = x_train[b * batch_size : (b+1) * batch_size]\n",
        "        y_true_batch = y_train[b * batch_size : (b+1) * batch_size, :]\n",
        "        y_train_cls_batch = y_train_cls[b * batch_size : (b+1) * batch_size]  # feed dict. for training\n",
        "        feed_dict_train = {x: x_batch,\n",
        "                          y_true: y_true_batch,\n",
        "                          dropout_rate : 0.4}        \n",
        "        opt, cls_pred = session.run([optimizer, y_pred_cls], feed_dict=feed_dict_train)\n",
        "        # Run the model to get predictions for test data\n",
        "        # Get true labels\n",
        "        cls_true = y_train_cls_batch\n",
        "        # Calculate the difference betweeb predictions and true labels\n",
        "        correct_prediction = np.equal(cls_pred, cls_true)\n",
        "        correct_pred_array.append( np.mean(correct_prediction)) # take the loss for each batch.\n",
        "        \n",
        "    return np.mean(correct_pred_array) #average the batch_loss array for epoch loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhxmS0YlCws_",
        "colab_type": "text"
      },
      "source": [
        "## Define test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDQQNV3-CwtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feed_dict_test = {x: x_test,    #feed placeholders with proper variables.\n",
        "                  y_true: y_test,\n",
        "                  y_true_cls: y_test_cls,\n",
        "                  dropout_rate : 0.01}  #test_training = 0 network should work for inference not for training\n",
        "def test():\n",
        "    # Run the model to get predictions for test data\n",
        "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
        "    \n",
        "    # Get true labels\n",
        "    cls_true = y_test_cls\n",
        "    \n",
        "    # Calculate the difference betweeb predictions and true labels\n",
        "    correct_prediction = np.equal(cls_pred, cls_true)\n",
        "    \n",
        "    # Calculate the total accuracy\n",
        "    acc = np.mean(correct_prediction) # Different from the train func., test func. \n",
        "    #doesnt avg. over total batch since it is given to network as a whole\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-McO8NLyCwtB",
        "colab_type": "text"
      },
      "source": [
        "## Performance before training\n",
        "\n",
        "The accuracy is expected to be around 10%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoG4lkKLCwtC",
        "colab_type": "code",
        "outputId": "acdef2ef-426d-4e11-c5c4-a31f192659e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(test())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9h8MDSzCwtD",
        "colab_type": "text"
      },
      "source": [
        "## Performance after training\n",
        "\n",
        "Measure training and test accuracy for at least 10 epochs, show it on a epoch/accuracy graph. The network is expected to reach around 70% accuracy at 10 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmRzcvlmCwtD",
        "colab_type": "code",
        "outputId": "4f1ce054-79df-4d43-ebac-12104dfff7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "epoch_number = 10\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "for a in range (epoch_number):\n",
        "    train_acc.append(train(batch_size = 128))\n",
        "    test_acc.append(test())\n",
        "print(\"final train acc : \")\n",
        "print(train_acc[-1])\n",
        "print(\"final test_acc : \")\n",
        "print(test_acc[-1])\n",
        "k = np.linspace(1,epoch_number,epoch_number)\n",
        "\n",
        "plt.plot(k,train_acc) #plot train and test accuracy graphs\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy on Training Data\")\n",
        "plt.show()\n",
        "\n",
        "plt.plot(k,test_acc)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy on Test Data\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final train acc : \n",
            "0.4229366987179487\n",
            "final test_acc : \n",
            "0.4642\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKhUIERGJ6v8BS1mxt8lAhxMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nInGpQIiISFwqECIiEpcKhIiIxPX/AWFV3/2GcLtBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYaVowXBCwtF",
        "colab_type": "text"
      },
      "source": [
        "## Close the session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT5BJO9BCwtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYTHNa_UCwtH",
        "colab_type": "text"
      },
      "source": [
        "## Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-25YKjHCwtI",
        "colab_type": "text"
      },
      "source": [
        "In this part of the 3rd question the network given in the figure is implemented using the low level API\n",
        "functions. Inputs are divided by 255 in order to place the numbers between 0-1. Dividing by 255 was rooted from the fact that inputs are\n",
        "pixel values of images. System consists of total a total of 3 convolution layers, 3 pooling layers each are placed after\n",
        "a convolution layer. After the third pooling layer 2 fully connected layers are present. Fully connected\n",
        "layers have 128 and 10 neurons respectively. Outside the network, a softmax activation function completes the \n",
        "network's guess for each image. Input size is not changed in conv. layers but downsampled to half of the input'size\n",
        "size in each pooling layers. \n",
        "In network definition, pooling type is not specified, AVG pooling is choosen in order to\n",
        "reduce information loss between layers. There is no difference in computation time observed between max pooling and avg \n",
        "pooling, both took ~50 secs. However,  final train_acc,test_acc = [0.40, 0.45] in max pooling while the\n",
        "final train_acc, test_acc = [0.42, 0.474] in average pooling, after 10 epochs. Here and later \"computation time\"\n",
        "refers to neither training, nor test time but a mixture of them. Time is calculated the execution duration of the 13th cell\n",
        "given by Google CoLab when mouse is hovered on the 'play' button, hw accelerator is choosen as GPU. \n",
        "\n",
        "Another unspecified point was the batch size, with batch_size 128, computation time for 10 epochs is ~50secs. and \n",
        "final test acc. is aroung 47%. When batch_size is reduced to 32, computation time for 10 epochs increased to 130 secs. while\n",
        "final test acc. increased to 57%. One reason of the accuracy increase [1] can be the fact that smaller batch sizes are noisy, \n",
        "offering a regularizing effect and lower generalization error.\n",
        "\n",
        "Dropout is implemented by using different dropout rates for test and training feed_dict's. Original dropout rates are \n",
        "0.4 and 0.01 for train and test respectively. When dropout is set to 0.99 for each function,test accuracy decreased to 0.1\n",
        "network doesn't learn anything during training and 1st fully connected layer neurons are not working, guess is completely randomized by output neurons.\n",
        "Later dropout is set to 0.99 and 0.01 for training and test respectively final test result was again 0.1. This time neurons were on for inference but,\n",
        "they were off while training hence, no learning observed. Turning off the neurons (dropout = 0.99 in both) did not changed the computation time.\n",
        "\n",
        "When epoch number is increased to 20 from 10, final training accuracy also increased to 61%.Moreover, computation time is also \n",
        "increased to 90secs.\n",
        "\n",
        "Learning rate was 0.0005 at first and network seems to be working fine but then i realize there are some confusing results.\n",
        "Confusing thing was, same network with same hyper parameters resulted in different final test accuracies in high-low level API versions. Then\n",
        " this kind of difference (70% vs. 20%) made me think about this but I couldn't find any difference in implementations apart from\n",
        "filter initialization (glorot vs. xavier) and batch normalization implementation (computation of scale,offset etc.) Then, learning rate increased to\n",
        "0.005(current value) and it is observed that final accuracy increases. Moreover, final accuracy increase as the epochs increase. Therefore, it can be inferred that\n",
        "network is working correctly. In the given implementation, test_acc examples are aroung 47% since i don't have time to re-tune my network. Actually I,\n",
        "observe that my network can reach %57 percent acc. with changing the epoch size while this report is being written.\n",
        "Long story short, observations on hyper-parameters does not resemble potential performance of the network because I started to write this report before fine tuning the network because I had very little time when I finally became sure about my network implementation.\n",
        "Hyper parameters are given as they give the final train_acc, test_acc = [0.42, 0.474] and changes mentioned in report applied on those parameters.\n",
        "\n",
        "[1] https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRjg7b97CwtJ",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] [Arm Community](https://community.arm.com)\n",
        "\n",
        "[2] https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
        "\n",
        "[3] https://gist.github.com/andrewkruger/d62faac06f950c4388d9b23aae0c9cc9#file-gistfile1-txt\n",
        "\n",
        "[4] https://github.com/METU-MMI-DeepLearning/MMI713_Deep_Learning/blob/master/Lab1.ipynb"
      ]
    }
  ]
}